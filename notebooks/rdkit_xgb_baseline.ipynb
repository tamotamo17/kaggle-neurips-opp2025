{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS 2025 - RDKit + XGBoost Baseline Model\n",
    "\n",
    "This notebook implements a comprehensive baseline model for the NeurIPS Open Polymer Prediction 2025 competition using:\n",
    "- RDKit for molecular descriptor generation\n",
    "- XGBoost for machine learning\n",
    "- Robust cross-validation strategy\n",
    "- Feature engineering optimizations\n",
    "\n",
    "Based on public competition approaches and best practices for molecular property prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RDKit imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Crippen, Lipinski, rdPartialCharges\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA, CalcLabuteASA, CalcNumRotatableBonds\n",
    "from rdkit.Chem.Descriptors import ExactMolWt, MolLogP, NumHDonors, NumHAcceptors\n",
    "from rdkit.Chem.rdmolops import GetFormalCharge\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "SUBMISSIONS_DIR = Path('../submissions')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "SUBMISSIONS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"RDKit version: {Chem.__version__ if hasattr(Chem, '__version__') else 'Available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (adjust file names based on actual competition data)\n",
    "def load_competition_data():\n",
    "    \"\"\"Load competition datasets\"\"\"\n",
    "    try:\n",
    "        # Try common file names\n",
    "        train_files = ['train.csv', 'training.csv', 'train_data.csv']\n",
    "        test_files = ['test.csv', 'testing.csv', 'test_data.csv']\n",
    "        \n",
    "        train_df = None\n",
    "        test_df = None\n",
    "        \n",
    "        for file in train_files:\n",
    "            if (DATA_DIR / file).exists():\n",
    "                train_df = pd.read_csv(DATA_DIR / file)\n",
    "                print(f\"Loaded training data from {file}\")\n",
    "                break\n",
    "        \n",
    "        for file in test_files:\n",
    "            if (DATA_DIR / file).exists():\n",
    "                test_df = pd.read_csv(DATA_DIR / file)\n",
    "                print(f\"Loaded test data from {file}\")\n",
    "                break\n",
    "        \n",
    "        if train_df is None or test_df is None:\n",
    "            print(\"Could not find data files. Please ensure data is downloaded.\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"Training data shape: {train_df.shape}\")\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load data\n",
    "train_df, test_df = load_competition_data()\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"\\nTraining data info:\")\n",
    "    print(train_df.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(train_df.head())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    print(train_df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\n⚠️ No data found. Creating dummy data for demonstration...\")\n",
    "    # Create dummy data for demonstration\n",
    "    dummy_smiles = [\n",
    "        'CCO',  # ethanol\n",
    "        'CC(C)O',  # isopropanol\n",
    "        'CCCCCCCCCCCCCCCCCC(=O)O',  # stearic acid\n",
    "        'c1ccccc1',  # benzene\n",
    "        'CC(C)(C)c1ccc(cc1)O'  # BHT\n",
    "    ]\n",
    "    \n",
    "    train_df = pd.DataFrame({\n",
    "        'id': range(100),\n",
    "        'smiles': np.random.choice(dummy_smiles, 100),\n",
    "        'target': np.random.normal(0, 1, 100)\n",
    "    })\n",
    "    \n",
    "    test_df = pd.DataFrame({\n",
    "        'id': range(100, 150),\n",
    "        'smiles': np.random.choice(dummy_smiles, 50)\n",
    "    })\n",
    "    \n",
    "    print(\"Created dummy data for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular Feature Engineering with RDKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_molecular_descriptors(smiles_list, descriptor_set='comprehensive'):\n",
    "    \"\"\"\n",
    "    Calculate molecular descriptors from SMILES strings using RDKit.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    smiles_list : list\n",
    "        List of SMILES strings\n",
    "    descriptor_set : str\n",
    "        Set of descriptors to calculate ('basic', 'comprehensive', 'all')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with molecular descriptors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define descriptor sets\n",
    "    basic_descriptors = {\n",
    "        'MolWt': Descriptors.MolWt,\n",
    "        'LogP': Descriptors.MolLogP,\n",
    "        'NumHDonors': Descriptors.NumHDonors,\n",
    "        'NumHAcceptors': Descriptors.NumHAcceptors,\n",
    "        'NumRotatableBonds': Descriptors.NumRotatableBonds,\n",
    "        'NumAromaticRings': Descriptors.NumAromaticRings,\n",
    "        'TPSA': Descriptors.TPSA,\n",
    "        'HeavyAtomCount': Descriptors.HeavyAtomCount,\n",
    "        'NumSaturatedRings': Descriptors.NumSaturatedRings,\n",
    "        'NumAliphaticRings': Descriptors.NumAliphaticRings\n",
    "    }\n",
    "    \n",
    "    comprehensive_descriptors = {\n",
    "        **basic_descriptors,\n",
    "        'BertzCT': Descriptors.BertzCT,\n",
    "        'Ipc': Descriptors.Ipc,\n",
    "        'Chi0v': Descriptors.Chi0v,\n",
    "        'Chi1v': Descriptors.Chi1v,\n",
    "        'Chi2v': Descriptors.Chi2v,\n",
    "        'Chi3v': Descriptors.Chi3v,\n",
    "        'Chi4v': Descriptors.Chi4v,\n",
    "        'Kappa1': Descriptors.Kappa1,\n",
    "        'Kappa2': Descriptors.Kappa2,\n",
    "        'Kappa3': Descriptors.Kappa3,\n",
    "        'LabuteASA': Descriptors.LabuteASA,\n",
    "        'BalabanJ': Descriptors.BalabanJ,\n",
    "        'PEOE_VSA1': Descriptors.PEOE_VSA1,\n",
    "        'PEOE_VSA2': Descriptors.PEOE_VSA2,\n",
    "        'PEOE_VSA3': Descriptors.PEOE_VSA3,\n",
    "        'SMR_VSA1': Descriptors.SMR_VSA1,\n",
    "        'SMR_VSA2': Descriptors.SMR_VSA2,\n",
    "        'SMR_VSA3': Descriptors.SMR_VSA3,\n",
    "        'SlogP_VSA1': Descriptors.SlogP_VSA1,\n",
    "        'SlogP_VSA2': Descriptors.SlogP_VSA2,\n",
    "        'SlogP_VSA3': Descriptors.SlogP_VSA3\n",
    "    }\n",
    "    \n",
    "    # Select descriptor set\n",
    "    if descriptor_set == 'basic':\n",
    "        descriptors = basic_descriptors\n",
    "    elif descriptor_set == 'comprehensive':\n",
    "        descriptors = comprehensive_descriptors\n",
    "    else:  # 'all'\n",
    "        # Get all available descriptors\n",
    "        descriptors = {name: getattr(Descriptors, name) for name in dir(Descriptors) \n",
    "                      if not name.startswith('_') and callable(getattr(Descriptors, name))}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Calculating {len(descriptors)} descriptors for {len(smiles_list)} molecules...\")\n",
    "    \n",
    "    for i, smiles in enumerate(tqdm(smiles_list)):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            if mol is not None:\n",
    "                # Calculate descriptors\n",
    "                mol_descriptors = {'SMILES': smiles}\n",
    "                \n",
    "                for desc_name, desc_func in descriptors.items():\n",
    "                    try:\n",
    "                        value = desc_func(mol)\n",
    "                        # Handle infinite or NaN values\n",
    "                        if np.isinf(value) or np.isnan(value):\n",
    "                            value = 0.0\n",
    "                        mol_descriptors[desc_name] = value\n",
    "                    except Exception as e:\n",
    "                        mol_descriptors[desc_name] = 0.0\n",
    "                \n",
    "                # Add custom descriptors\n",
    "                mol_descriptors['NumRings'] = mol.GetRingInfo().NumRings()\n",
    "                mol_descriptors['NumAtoms'] = mol.GetNumAtoms()\n",
    "                mol_descriptors['NumBonds'] = mol.GetNumBonds()\n",
    "                mol_descriptors['NumHeteroatoms'] = Descriptors.NumHeteroatoms(mol)\n",
    "                mol_descriptors['FractionCsp3'] = Descriptors.FractionCsp3(mol)\n",
    "                \n",
    "                # Lipinski's Rule of Five\n",
    "                mol_descriptors['Lipinski_Violations'] = (\n",
    "                    (mol_descriptors['MolWt'] > 500) +\n",
    "                    (mol_descriptors['LogP'] > 5) +\n",
    "                    (mol_descriptors['NumHDonors'] > 5) +\n",
    "                    (mol_descriptors['NumHAcceptors'] > 10)\n",
    "                )\n",
    "                \n",
    "                results.append(mol_descriptors)\n",
    "            else:\n",
    "                # Invalid SMILES\n",
    "                invalid_mol = {'SMILES': smiles}\n",
    "                for desc_name in descriptors.keys():\n",
    "                    invalid_mol[desc_name] = 0.0\n",
    "                results.append(invalid_mol)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SMILES {smiles}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    feature_df = pd.DataFrame(results)\n",
    "    print(f\"Generated {feature_df.shape[1]-1} molecular descriptors\")\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# Calculate descriptors for training and test data\n",
    "# Identify SMILES column (adjust based on actual column name)\n",
    "smiles_col = None\n",
    "for col in train_df.columns:\n",
    "    if 'smiles' in col.lower() or 'molecule' in col.lower():\n",
    "        smiles_col = col\n",
    "        break\n",
    "\n",
    "if smiles_col is None:\n",
    "    print(\"Could not identify SMILES column. Please specify manually.\")\n",
    "    # Assume first non-numeric column is SMILES\n",
    "    smiles_col = train_df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "print(f\"Using SMILES column: {smiles_col}\")\n",
    "\n",
    "# Generate features\n",
    "train_features = calculate_molecular_descriptors(train_df[smiles_col].tolist(), 'comprehensive')\n",
    "test_features = calculate_molecular_descriptors(test_df[smiles_col].tolist(), 'comprehensive')\n",
    "\n",
    "print(f\"\\nTraining features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_feature_engineering(features_df):\n",
    "    \"\"\"\n",
    "    Apply advanced feature engineering techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with molecular descriptors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    df = features_df.copy()\n",
    "    \n",
    "    # Remove SMILES column for feature engineering\n",
    "    if 'SMILES' in df.columns:\n",
    "        df = df.drop('SMILES', axis=1)\n",
    "    \n",
    "    # Create ratio features\n",
    "    df['MolWt_per_Atom'] = df['MolWt'] / (df['NumAtoms'] + 1e-6)\n",
    "    df['TPSA_per_Atom'] = df['TPSA'] / (df['NumAtoms'] + 1e-6)\n",
    "    df['LogP_per_Atom'] = df['LogP'] / (df['NumAtoms'] + 1e-6)\n",
    "    df['HeavyAtom_Ratio'] = df['HeavyAtomCount'] / (df['NumAtoms'] + 1e-6)\n",
    "    df['Aromatic_Ratio'] = df['NumAromaticRings'] / (df['NumRings'] + 1e-6)\n",
    "    df['Rotatable_Ratio'] = df['NumRotatableBonds'] / (df['NumBonds'] + 1e-6)\n",
    "    df['HBond_Ratio'] = (df['NumHDonors'] + df['NumHAcceptors']) / (df['NumAtoms'] + 1e-6)\n",
    "    \n",
    "    # Create interaction features\n",
    "    df['LogP_TPSA'] = df['LogP'] * df['TPSA']\n",
    "    df['MolWt_LogP'] = df['MolWt'] * df['LogP']\n",
    "    df['Rings_Aromatic'] = df['NumRings'] * df['NumAromaticRings']\n",
    "    df['HBond_TPSA'] = (df['NumHDonors'] + df['NumHAcceptors']) * df['TPSA']\n",
    "    \n",
    "    # Create polynomial features for key descriptors\n",
    "    key_features = ['MolWt', 'LogP', 'TPSA', 'NumRotatableBonds']\n",
    "    for feat in key_features:\n",
    "        if feat in df.columns:\n",
    "            df[f'{feat}_squared'] = df[feat] ** 2\n",
    "            df[f'{feat}_sqrt'] = np.sqrt(np.abs(df[feat]))\n",
    "            df[f'{feat}_log'] = np.log1p(np.abs(df[feat]))\n",
    "    \n",
    "    # Create binned features\n",
    "    df['MolWt_bin'] = pd.cut(df['MolWt'], bins=10, labels=False)\n",
    "    df['LogP_bin'] = pd.cut(df['LogP'], bins=10, labels=False)\n",
    "    df['TPSA_bin'] = pd.cut(df['TPSA'], bins=10, labels=False)\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    print(f\"Feature engineering complete. New shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_features_eng = advanced_feature_engineering(train_features)\n",
    "test_features_eng = advanced_feature_engineering(test_features)\n",
    "\n",
    "print(f\"\\nEngineered training features shape: {train_features_eng.shape}\")\n",
    "print(f\"Engineered test features shape: {test_features_eng.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "target_col = None\n",
    "for col in train_df.columns:\n",
    "    if any(term in col.lower() for term in ['target', 'property', 'value', 'y', 'label']):\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # Assume last numeric column is target\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    target_col = numeric_cols[-1]\n",
    "\n",
    "print(f\"Using target column: {target_col}\")\n",
    "\n",
    "# Extract target values\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "# Analyze target distribution\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Count: {len(y_train)}\")\n",
    "print(f\"  Mean: {y_train.mean():.4f}\")\n",
    "print(f\"  Std: {y_train.std():.4f}\")\n",
    "print(f\"  Min: {y_train.min():.4f}\")\n",
    "print(f\"  Max: {y_train.max():.4f}\")\n",
    "print(f\"  Skewness: {y_train.skew():.4f}\")\n",
    "print(f\"  Kurtosis: {y_train.kurtosis():.4f}\")\n",
    "\n",
    "# Plot target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].set_xlabel(target_col)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y_train)\n",
    "axes[1].set_title('Target Box Plot')\n",
    "axes[1].set_ylabel(target_col)\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(y_train, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normal)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers\n",
    "q1 = y_train.quantile(0.25)\n",
    "q3 = y_train.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = ((y_train < lower_bound) | (y_train > upper_bound)).sum()\n",
    "\n",
    "print(f\"\\nOutlier analysis:\")\n",
    "print(f\"  IQR: {iqr:.4f}\")\n",
    "print(f\"  Lower bound: {lower_bound:.4f}\")\n",
    "print(f\"  Upper bound: {upper_bound:.4f}\")\n",
    "print(f\"  Number of outliers: {outliers} ({outliers/len(y_train)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "def remove_constant_features(X_train, X_test):\n",
    "    \"\"\"Remove features with zero variance\"\"\"\n",
    "    constant_features = []\n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].nunique() <= 1:\n",
    "            constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"Removing {len(constant_features)} constant features\")\n",
    "        X_train = X_train.drop(columns=constant_features)\n",
    "        X_test = X_test.drop(columns=constant_features)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Remove highly correlated features\n",
    "def remove_correlated_features(X_train, X_test, threshold=0.95):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"Removing {len(to_drop)} highly correlated features (threshold: {threshold})\")\n",
    "        X_train = X_train.drop(columns=to_drop)\n",
    "        X_test = X_test.drop(columns=to_drop)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_features_eng.copy()\n",
    "X_test = test_features_eng.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "print(f\"Initial feature shapes: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Remove constant features\n",
    "X_train, X_test = remove_constant_features(X_train, X_test)\n",
    "\n",
    "# Remove highly correlated features\n",
    "X_train, X_test = remove_correlated_features(X_train, X_test, threshold=0.95)\n",
    "\n",
    "print(f\"After feature removal: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Feature selection using statistical tests\n",
    "n_features = min(200, X_train.shape[1])  # Select top 200 features or all if fewer\n",
    "selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "X_train_selected = selector.fit_transform(X_train, y)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(f\"\\nSelected {len(selected_features)} features using statistical tests\")\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_final = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "X_test_final = pd.DataFrame(X_test_selected, columns=selected_features)\n",
    "\n",
    "print(f\"Final feature shapes: Train {X_train_final.shape}, Test {X_test_final.shape}\")\n",
    "\n",
    "# Display top selected features\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'score': feature_scores\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 selected features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameters (optimized for molecular property prediction)\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_folds = 5\n",
    "kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Cross-validation training\n",
    "cv_scores = []\n",
    "cv_models = []\n",
    "oof_predictions = np.zeros(len(X_train_final))\n",
    "test_predictions = np.zeros(len(X_test_final))\n",
    "\n",
    "print(\"Starting cross-validation training...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_final)):\n",
    "    print(f\"\\nFold {fold + 1}/{cv_folds}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train, X_fold_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        eval_set=[(X_fold_val, y_fold_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = model.predict(X_fold_val)\n",
    "    test_pred = model.predict(X_test_final)\n",
    "    \n",
    "    # Store results\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    test_predictions += test_pred / cv_folds\n",
    "    cv_models.append(model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_fold_val, val_pred))\n",
    "    fold_mae = mean_absolute_error(y_fold_val, val_pred)\n",
    "    fold_r2 = r2_score(y_fold_val, val_pred)\n",
    "    \n",
    "    cv_scores.append(fold_rmse)\n",
    "    \n",
    "    print(f\"  RMSE: {fold_rmse:.6f}\")\n",
    "    print(f\"  MAE: {fold_mae:.6f}\")\n",
    "    print(f\"  R²: {fold_r2:.6f}\")\n",
    "\n",
    "# Overall CV performance\n",
    "cv_rmse = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "oof_mae = mean_absolute_error(y, oof_predictions)\n",
    "oof_r2 = r2_score(y, oof_predictions)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"CROSS-VALIDATION RESULTS:\")\n",
    "print(f\"  CV RMSE: {cv_rmse:.6f} ± {cv_std:.6f}\")\n",
    "print(f\"  OOF RMSE: {oof_rmse:.6f}\")\n",
    "print(f\"  OOF MAE: {oof_mae:.6f}\")\n",
    "print(f\"  OOF R²: {oof_r2:.6f}\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y, oof_predictions, alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'Out-of-Fold Predictions (R² = {oof_r2:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Residual analysis\n",
    "residuals = y - oof_predictions\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(oof_predictions, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across all folds\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'importance': np.mean([model.feature_importances_ for model in cv_models], axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_df.to_csv(MODELS_DIR / 'feature_importance.csv', index=False)\n",
    "print(f\"\\nFeature importance saved to {MODELS_DIR / 'feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects\n",
    "model_artifacts = {\n",
    "    'models': cv_models,\n",
    "    'feature_selector': selector,\n",
    "    'selected_features': selected_features.tolist(),\n",
    "    'cv_scores': cv_scores,\n",
    "    'oof_predictions': oof_predictions,\n",
    "    'test_predictions': test_predictions,\n",
    "    'target_column': target_col,\n",
    "    'smiles_column': smiles_col\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, MODELS_DIR / 'xgb_baseline_models.pkl')\n",
    "print(f\"Models saved to {MODELS_DIR / 'xgb_baseline_models.pkl'}\")\n",
    "\n",
    "# Generate submission file\n",
    "def create_submission(test_df, predictions, submission_path):\n",
    "    \"\"\"Create submission file\"\"\"\n",
    "    # Identify ID column\n",
    "    id_col = None\n",
    "    for col in test_df.columns:\n",
    "        if 'id' in col.lower():\n",
    "            id_col = col\n",
    "            break\n",
    "    \n",
    "    if id_col is None:\n",
    "        # Use index as ID\n",
    "        submission_df = pd.DataFrame({\n",
    "            'id': range(len(predictions)),\n",
    "            'prediction': predictions\n",
    "        })\n",
    "    else:\n",
    "        submission_df = pd.DataFrame({\n",
    "            id_col: test_df[id_col],\n",
    "            'prediction': predictions\n",
    "        })\n",
    "    \n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    return submission_df\n",
    "\n",
    "# Create submission\n",
    "submission_path = SUBMISSIONS_DIR / 'xgb_baseline_submission.csv'\n",
    "submission_df = create_submission(test_df, test_predictions, submission_path)\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"  Mean prediction: {test_predictions.mean():.6f}\")\n",
    "print(f\"  Std prediction: {test_predictions.std():.6f}\")\n",
    "print(f\"  Min prediction: {test_predictions.min():.6f}\")\n",
    "print(f\"  Max prediction: {test_predictions.max():.6f}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model performance\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 DATASET INFORMATION:\")\n",
    "print(f\"  • Training samples: {len(X_train_final)}\")\n",
    "print(f\"  • Test samples: {len(X_test_final)}\")\n",
    "print(f\"  • Features used: {X_train_final.shape[1]}\")\n",
    "print(f\"  • Target column: {target_col}\")\n",
    "print(f\"  • SMILES column: {smiles_col}\")\n",
    "\n",
    "print(f\"\\n🎯 MODEL PERFORMANCE:\")\n",
    "print(f\"  • Cross-validation RMSE: {cv_rmse:.6f} ± {cv_std:.6f}\")\n",
    "print(f\"  • Out-of-fold RMSE: {oof_rmse:.6f}\")\n",
    "print(f\"  • Out-of-fold MAE: {oof_mae:.6f}\")\n",
    "print(f\"  • Out-of-fold R²: {oof_r2:.6f}\")\n",
    "\n",
    "print(f\"\\n🔬 FEATURE ENGINEERING:\")\n",
    "print(f\"  • Total molecular descriptors: {train_features.shape[1]-1}\")\n",
    "print(f\"  • Engineered features: {train_features_eng.shape[1]}\")\n",
    "print(f\"  • Selected features: {X_train_final.shape[1]}\")\n",
    "print(f\"  • Top feature: {feature_importance_df.iloc[0]['feature']}\")\n",
    "\n",
    "print(f\"\\n📈 PREDICTIONS:\")\n",
    "print(f\"  • Mean prediction: {test_predictions.mean():.6f}\")\n",
    "print(f\"  • Prediction std: {test_predictions.std():.6f}\")\n",
    "print(f\"  • Prediction range: [{test_predictions.min():.6f}, {test_predictions.max():.6f}]\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"  • Model shows {'good' if oof_r2 > 0.7 else 'moderate' if oof_r2 > 0.5 else 'poor'} predictive performance\")\n",
    "print(f\"  • Feature selection reduced dimensionality by {((train_features_eng.shape[1] - X_train_final.shape[1]) / train_features_eng.shape[1] * 100):.1f}%\")\n",
    "print(f\"  • Cross-validation stability: {'good' if cv_std < cv_rmse * 0.1 else 'moderate'}\")\n",
    "\n",
    "print(f\"\\n📁 SAVED FILES:\")\n",
    "print(f\"  • Models: {MODELS_DIR / 'xgb_baseline_models.pkl'}\")\n",
    "print(f\"  • Feature importance: {MODELS_DIR / 'feature_importance.csv'}\")\n",
    "print(f\"  • Submission: {submission_path}\")\n",
    "\n",
    "print(\"\\n✅ Baseline model training complete!\")\n",
    "print(\"\\n🚀 Next steps:\")\n",
    "print(\"  1. Hyperparameter tuning with Optuna or similar\")\n",
    "print(\"  2. Try other algorithms (LightGBM, CatBoost, Neural Networks)\")\n",
    "print(\"  3. Ensemble different models\")\n",
    "print(\"  4. Advanced feature engineering (molecular fingerprints, graph features)\")\n",
    "print(\"  5. External data augmentation\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}