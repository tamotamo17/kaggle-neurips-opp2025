{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS Open Polymer Prediction 2025 - Comprehensive EDA\n",
    "\n",
    "This notebook provides a thorough exploratory data analysis for the polymer prediction competition.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading & Overview](#data-loading)\n",
    "2. [Basic Statistics](#basic-stats)\n",
    "3. [Target Variable Analysis](#target-analysis)\n",
    "4. [Molecular Structure Analysis](#molecular-analysis)\n",
    "5. [Feature Engineering Ideas](#feature-engineering)\n",
    "6. [Data Quality Assessment](#data-quality)\n",
    "7. [Correlation Analysis](#correlation)\n",
    "8. [Distribution Analysis](#distributions)\n",
    "9. [Outlier Detection](#outliers)\n",
    "10. [Insights & Next Steps](#insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemistry libraries\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, rdMolDescriptors, Draw\n",
    "    from rdkit.Chem.Draw import IPythonConsole\n",
    "    RDKIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"RDKit not available - molecular analysis will be limited\")\n",
    "    RDKIT_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from mordred import Calculator, descriptors\n",
    "    MORDRED_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Mordred not available - descriptor calculation will be limited\")\n",
    "    MORDRED_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path('../data')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"RDKit available: {RDKIT_AVAILABLE}\")\n",
    "print(f\"Mordred available: {MORDRED_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Overview {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data files\n",
    "data_files = list(DATA_DIR.glob('*.csv'))\n",
    "print(\"Available data files:\")\n",
    "for file in data_files:\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "if not data_files:\n",
    "    print(\"\\nNo data files found. Please download the competition data first.\")\n",
    "    print(\"Run: uv run kaggle competitions download -c neurips-open-polymer-prediction-2025 -p data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (adjust file names based on actual competition data)\n",
    "# Common file patterns for Kaggle competitions\n",
    "possible_files = {\n",
    "    'train': ['train.csv', 'training.csv', 'train_data.csv'],\n",
    "    'test': ['test.csv', 'testing.csv', 'test_data.csv'],\n",
    "    'sample_submission': ['sample_submission.csv', 'submission.csv']\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for dataset_type, file_patterns in possible_files.items():\n",
    "    for pattern in file_patterns:\n",
    "        file_path = DATA_DIR / pattern\n",
    "        if file_path.exists():\n",
    "            print(f\"Loading {dataset_type}: {pattern}\")\n",
    "            datasets[dataset_type] = pd.read_csv(file_path)\n",
    "            break\n",
    "    else:\n",
    "        print(f\"No {dataset_type} file found\")\n",
    "\n",
    "# Display basic info about loaded datasets\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} DATASET:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics {#basic-stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for training data\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    \n",
    "    print(\"TRAINING DATA OVERVIEW:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(train_df.head())\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nDATA TYPES:\")\n",
    "    print(train_df.dtypes.value_counts())\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMISSING VALUES:\")\n",
    "    missing = train_df.isnull().sum()\n",
    "    missing_pct = (missing / len(train_df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Unique values count\n",
    "    print(\"\\nUNIQUE VALUES:\")\n",
    "    unique_counts = train_df.nunique().sort_values(ascending=False)\n",
    "    display(unique_counts.head(10))\n",
    "else:\n",
    "    print(\"Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis {#target-analysis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential target columns\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    \n",
    "    # Common target column names for polymer prediction\n",
    "    potential_targets = ['target', 'property', 'value', 'measurement', 'y', 'label']\n",
    "    \n",
    "    # Find numeric columns that could be targets\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(\"POTENTIAL TARGET COLUMNS:\")\n",
    "    print(\"Numeric columns:\", numeric_cols)\n",
    "    \n",
    "    # If we can identify target columns, analyze them\n",
    "    # This section will be updated based on actual data structure\n",
    "    target_cols = [col for col in numeric_cols if any(target_name in col.lower() for target_name in potential_targets)]\n",
    "    \n",
    "    if target_cols:\n",
    "        print(f\"\\nIdentified target columns: {target_cols}\")\n",
    "        \n",
    "        for target_col in target_cols:\n",
    "            print(f\"\\nANALYSIS FOR {target_col.upper()}:\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            target_data = train_df[target_col].dropna()\n",
    "            \n",
    "            # Basic statistics\n",
    "            print(f\"Count: {len(target_data)}\")\n",
    "            print(f\"Mean: {target_data.mean():.4f}\")\n",
    "            print(f\"Std: {target_data.std():.4f}\")\n",
    "            print(f\"Min: {target_data.min():.4f}\")\n",
    "            print(f\"Max: {target_data.max():.4f}\")\n",
    "            print(f\"Skewness: {target_data.skew():.4f}\")\n",
    "            print(f\"Kurtosis: {target_data.kurtosis():.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo obvious target columns identified. Please specify target columns manually.\")\n",
    "        print(\"All numeric columns:\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  - {col}: {train_df[col].describe().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distributions\n",
    "def plot_target_distribution(data, column_name):\n",
    "    \"\"\"Plot distribution of target variable\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Distribution Analysis: {column_name}', fontsize=16)\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Histogram')\n",
    "    axes[0, 0].set_xlabel(column_name)\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 1].boxplot(data)\n",
    "    axes[0, 1].set_title('Box Plot')\n",
    "    axes[0, 1].set_ylabel(column_name)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot (Normal)')\n",
    "    \n",
    "    # Log scale histogram (if all values are positive)\n",
    "    if (data > 0).all():\n",
    "        axes[1, 1].hist(np.log(data), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_title('Log-transformed Histogram')\n",
    "        axes[1, 1].set_xlabel(f'log({column_name})')\n",
    "    else:\n",
    "        axes[1, 1].hist(data, bins=50, alpha=0.7, edgecolor='black', cumulative=True, density=True)\n",
    "        axes[1, 1].set_title('Cumulative Distribution')\n",
    "        axes[1, 1].set_xlabel(column_name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for identified target columns\n",
    "if 'train' in datasets and target_cols:\n",
    "    for target_col in target_cols:\n",
    "        target_data = train_df[target_col].dropna()\n",
    "        plot_target_distribution(target_data, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Molecular Structure Analysis {#molecular-analysis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular structure analysis\n",
    "if 'train' in datasets and RDKIT_AVAILABLE:\n",
    "    train_df = datasets['train']\n",
    "    \n",
    "    # Look for SMILES or molecular structure columns\n",
    "    potential_smiles_cols = [col for col in train_df.columns if any(term in col.lower() for term in ['smiles', 'molecule', 'structure', 'mol'])]\n",
    "    \n",
    "    print(\"MOLECULAR STRUCTURE ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Potential SMILES columns: {potential_smiles_cols}\")\n",
    "    \n",
    "    if potential_smiles_cols:\n",
    "        smiles_col = potential_smiles_cols[0]  # Use first found column\n",
    "        smiles_data = train_df[smiles_col].dropna().unique()\n",
    "        \n",
    "        print(f\"\\nAnalyzing column: {smiles_col}\")\n",
    "        print(f\"Unique molecules: {len(smiles_data)}\")\n",
    "        print(f\"Sample SMILES: {smiles_data[:5].tolist()}\")\n",
    "        \n",
    "        # Validate SMILES\n",
    "        valid_smiles = []\n",
    "        invalid_count = 0\n",
    "        \n",
    "        for smiles in smiles_data[:100]:  # Check first 100 for speed\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    valid_smiles.append(smiles)\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "            except:\n",
    "                invalid_count += 1\n",
    "        \n",
    "        print(f\"\\nSMILES validation (first 100):\")\n",
    "        print(f\"Valid SMILES: {len(valid_smiles)}\")\n",
    "        print(f\"Invalid SMILES: {invalid_count}\")\n",
    "        \n",
    "        # Calculate basic molecular properties\n",
    "        if valid_smiles:\n",
    "            mol_properties = []\n",
    "            \n",
    "            for smiles in valid_smiles[:50]:  # Analyze first 50 valid molecules\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    props = {\n",
    "                        'SMILES': smiles,\n",
    "                        'MolWt': Descriptors.MolWt(mol),\n",
    "                        'LogP': Descriptors.MolLogP(mol),\n",
    "                        'NumHDonors': Descriptors.NumHDonors(mol),\n",
    "                        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "                        'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
    "                        'NumAromaticRings': Descriptors.NumAromaticRings(mol),\n",
    "                        'TPSA': Descriptors.TPSA(mol),\n",
    "                        'NumAtoms': mol.GetNumAtoms(),\n",
    "                        'NumBonds': mol.GetNumBonds()\n",
    "                    }\n",
    "                    mol_properties.append(props)\n",
    "            \n",
    "            if mol_properties:\n",
    "                mol_df = pd.DataFrame(mol_properties)\n",
    "                print(\"\\nMOLECULAR PROPERTIES SUMMARY:\")\n",
    "                display(mol_df.describe())\n",
    "                \n",
    "                # Plot molecular property distributions\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "                fig.suptitle('Molecular Property Distributions', fontsize=16)\n",
    "                \n",
    "                axes[0, 0].hist(mol_df['MolWt'], bins=20, alpha=0.7)\n",
    "                axes[0, 0].set_title('Molecular Weight')\n",
    "                axes[0, 0].set_xlabel('Molecular Weight (Da)')\n",
    "                \n",
    "                axes[0, 1].hist(mol_df['LogP'], bins=20, alpha=0.7)\n",
    "                axes[0, 1].set_title('LogP')\n",
    "                axes[0, 1].set_xlabel('LogP')\n",
    "                \n",
    "                axes[1, 0].hist(mol_df['NumAtoms'], bins=20, alpha=0.7)\n",
    "                axes[1, 0].set_title('Number of Atoms')\n",
    "                axes[1, 0].set_xlabel('Number of Atoms')\n",
    "                \n",
    "                axes[1, 1].hist(mol_df['TPSA'], bins=20, alpha=0.7)\n",
    "                axes[1, 1].set_title('Topological Polar Surface Area')\n",
    "                axes[1, 1].set_xlabel('TPSA (Å²)')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    else:\n",
    "        print(\"No molecular structure columns found\")\n",
    "else:\n",
    "    if not RDKIT_AVAILABLE:\n",
    "        print(\"RDKit not available - skipping molecular analysis\")\n",
    "    else:\n",
    "        print(\"Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Ideas {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering suggestions based on polymer prediction\n",
    "print(\"FEATURE ENGINEERING IDEAS FOR POLYMER PREDICTION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_ideas = {\n",
    "    \"Molecular Descriptors\": [\n",
    "        \"Molecular weight and size descriptors\",\n",
    "        \"Topological descriptors (connectivity indices)\",\n",
    "        \"Electronic descriptors (HOMO/LUMO energies)\",\n",
    "        \"Geometric descriptors (surface area, volume)\",\n",
    "        \"Pharmacophore descriptors\"\n",
    "    ],\n",
    "    \"Chemical Properties\": [\n",
    "        \"Lipophilicity (LogP, LogD)\",\n",
    "        \"Solubility parameters\",\n",
    "        \"Hydrogen bonding capacity\",\n",
    "        \"Aromaticity measures\",\n",
    "        \"Flexibility indices\"\n",
    "    ],\n",
    "    \"Polymer-Specific Features\": [\n",
    "        \"Monomer composition ratios\",\n",
    "        \"Chain length indicators\",\n",
    "        \"Cross-linking density measures\",\n",
    "        \"Glass transition temperature predictors\",\n",
    "        \"Crystallinity indices\"\n",
    "    ],\n",
    "    \"Structural Features\": [\n",
    "        \"Ring counts and types\",\n",
    "        \"Functional group counts\",\n",
    "        \"Branch points and chain ends\",\n",
    "        \"Stereochemistry descriptors\",\n",
    "        \"Atom type frequencies\"\n",
    "    ],\n",
    "    \"Interaction Features\": [\n",
    "        \"Molecular similarity matrices\",\n",
    "        \"Tanimoto coefficients\",\n",
    "        \"Pharmacophore similarities\",\n",
    "        \"Shape similarities\",\n",
    "        \"Electrostatic similarities\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, ideas in feature_ideas.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for idea in ideas:\n",
    "        print(f\"  â€¢ {idea}\")\n",
    "\n",
    "print(\"\\nFEATURE GENERATION TOOLS:\")\n",
    "print(\"â€¢ RDKit: Comprehensive molecular descriptors\")\n",
    "print(\"â€¢ Mordred: Extended molecular descriptors (>1800 descriptors)\")\n",
    "print(\"â€¢ PyBioMed: Biomolecular descriptors\")\n",
    "print(\"â€¢ ChemML: Machine learning for chemistry\")\n",
    "print(\"â€¢ DeepChem: Deep learning molecular features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment {#data-quality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    \n",
    "    print(\"DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    print(f\"Total rows: {len(train_df)}\")\n",
    "    print(f\"Duplicate rows: {train_df.duplicated().sum()}\")\n",
    "    \n",
    "    # Missing data patterns\n",
    "    missing_patterns = train_df.isnull().value_counts().head(10)\n",
    "    print(f\"\\nMost common missing data patterns:\")\n",
    "    for pattern, count in missing_patterns.items():\n",
    "        print(f\"  {pattern}: {count} rows ({count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Data type consistency\n",
    "    print(f\"\\nData type distribution:\")\n",
    "    dtype_counts = train_df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Potential data issues\n",
    "    print(f\"\\nPOTENTIAL DATA ISSUES:\")\n",
    "    \n",
    "    # Check for constant columns\n",
    "    constant_cols = [col for col in train_df.columns if train_df[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"  â€¢ Constant columns: {constant_cols}\")\n",
    "    \n",
    "    # Check for high cardinality categorical columns\n",
    "    categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
    "    high_cardinality = [col for col in categorical_cols if train_df[col].nunique() > len(train_df) * 0.5]\n",
    "    if high_cardinality:\n",
    "        print(f\"  â€¢ High cardinality categorical columns: {high_cardinality}\")\n",
    "    \n",
    "    # Check for potential ID columns\n",
    "    potential_ids = [col for col in train_df.columns if train_df[col].nunique() == len(train_df)]\n",
    "    if potential_ids:\n",
    "        print(f\"  â€¢ Potential ID columns: {potential_ids}\")\n",
    "    \n",
    "    # Check for extreme outliers in numeric columns\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    outlier_cols = []\n",
    "    for col in numeric_cols:\n",
    "        q1 = train_df[col].quantile(0.25)\n",
    "        q3 = train_df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 3 * iqr\n",
    "        upper_bound = q3 + 3 * iqr\n",
    "        outliers = ((train_df[col] < lower_bound) | (train_df[col] > upper_bound)).sum()\n",
    "        if outliers > len(train_df) * 0.05:  # More than 5% outliers\n",
    "            outlier_cols.append((col, outliers))\n",
    "    \n",
    "    if outlier_cols:\n",
    "        print(f\"  â€¢ Columns with many outliers:\")\n",
    "        for col, count in outlier_cols:\n",
    "            print(f\"    - {col}: {count} outliers ({count/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis {#correlation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        print(\"CORRELATION ANALYSIS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = train_df[numeric_cols].corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                    square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Correlation Matrix of Numeric Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highly correlated pairs\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:  # High correlation threshold\n",
    "                    high_corr_pairs.append((\n",
    "                        corr_matrix.columns[i],\n",
    "                        corr_matrix.columns[j],\n",
    "                        corr_val\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(f\"\\nHIGHLY CORRELATED PAIRS (|r| > 0.7):\")\n",
    "            for col1, col2, corr_val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"  {col1} â†” {col2}: {corr_val:.3f}\")\n",
    "        else:\n",
    "            print(f\"\\nNo highly correlated pairs found (|r| > 0.7)\")\n",
    "        \n",
    "        # Correlation with target variables\n",
    "        if target_cols:\n",
    "            print(f\"\\nCORRELATION WITH TARGET VARIABLES:\")\n",
    "            for target_col in target_cols:\n",
    "                target_corr = corr_matrix[target_col].abs().sort_values(ascending=False)\n",
    "                print(f\"\\nTop correlations with {target_col}:\")\n",
    "                for col, corr_val in target_corr.head(10).items():\n",
    "                    if col != target_col:\n",
    "                        print(f\"  {col}: {train_df[col].corr(train_df[target_col]):.3f}\")\n",
    "    else:\n",
    "        print(\"Insufficient numeric columns for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distribution Analysis {#distributions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis for numeric variables\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    print(\"DISTRIBUTION ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze distributions\n",
    "    distribution_stats = []\n",
    "    \n",
    "    for col in numeric_cols[:10]:  # Analyze first 10 numeric columns\n",
    "        data = train_df[col].dropna()\n",
    "        if len(data) > 0:\n",
    "            stats_dict = {\n",
    "                'Column': col,\n",
    "                'Mean': data.mean(),\n",
    "                'Median': data.median(),\n",
    "                'Std': data.std(),\n",
    "                'Skewness': data.skew(),\n",
    "                'Kurtosis': data.kurtosis(),\n",
    "                'Min': data.min(),\n",
    "                'Max': data.max(),\n",
    "                'Range': data.max() - data.min(),\n",
    "                'CV': data.std() / data.mean() if data.mean() != 0 else np.inf\n",
    "            }\n",
    "            distribution_stats.append(stats_dict)\n",
    "    \n",
    "    if distribution_stats:\n",
    "        dist_df = pd.DataFrame(distribution_stats)\n",
    "        print(\"\\nDISTRIBUTION STATISTICS:\")\n",
    "        display(dist_df.round(4))\n",
    "        \n",
    "        # Plot distributions for selected columns\n",
    "        cols_to_plot = numeric_cols[:6]  # Plot first 6 numeric columns\n",
    "        if len(cols_to_plot) > 0:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(cols_to_plot):\n",
    "                if i < len(axes):\n",
    "                    data = train_df[col].dropna()\n",
    "                    axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{col}\\nSkew: {data.skew():.2f}, Kurt: {data.kurtosis():.2f}')\n",
    "                    axes[i].set_xlabel(col)\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for i in range(len(cols_to_plot), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle('Distribution of Numeric Variables', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Identify potentially problematic distributions\n",
    "        print(\"\\nDISTRIBUTION ISSUES:\")\n",
    "        high_skew = dist_df[abs(dist_df['Skewness']) > 2]['Column'].tolist()\n",
    "        if high_skew:\n",
    "            print(f\"  â€¢ Highly skewed columns (|skew| > 2): {high_skew}\")\n",
    "        \n",
    "        high_kurtosis = dist_df[abs(dist_df['Kurtosis']) > 7]['Column'].tolist()\n",
    "        if high_kurtosis:\n",
    "            print(f\"  â€¢ High kurtosis columns (|kurt| > 7): {high_kurtosis}\")\n",
    "        \n",
    "        high_cv = dist_df[dist_df['CV'] > 2]['Column'].tolist()\n",
    "        if high_cv:\n",
    "            print(f\"  â€¢ High variability columns (CV > 2): {high_cv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection {#outliers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "if 'train' in datasets:\n",
    "    train_df = datasets['train']\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    print(\"OUTLIER DETECTION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in numeric_cols[:10]:  # Analyze first 10 numeric columns\n",
    "        data = train_df[col].dropna()\n",
    "        if len(data) > 0:\n",
    "            # IQR method\n",
    "            q1 = data.quantile(0.25)\n",
    "            q3 = data.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Outlier bounds\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            extreme_lower = q1 - 3 * iqr\n",
    "            extreme_upper = q3 + 3 * iqr\n",
    "            \n",
    "            # Count outliers\n",
    "            mild_outliers = ((data < lower_bound) & (data >= extreme_lower)).sum() + \\\n",
    "                          ((data > upper_bound) & (data <= extreme_upper)).sum()\n",
    "            extreme_outliers = (data < extreme_lower).sum() + (data > extreme_upper).sum()\n",
    "            \n",
    "            # Z-score method\n",
    "            z_scores = np.abs((data - data.mean()) / data.std())\n",
    "            z_outliers_2 = (z_scores > 2).sum()\n",
    "            z_outliers_3 = (z_scores > 3).sum()\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Total_Points': len(data),\n",
    "                'Mild_Outliers_IQR': mild_outliers,\n",
    "                'Extreme_Outliers_IQR': extreme_outliers,\n",
    "                'Z_Outliers_2std': z_outliers_2,\n",
    "                'Z_Outliers_3std': z_outliers_3,\n",
    "                'Outlier_Pct_IQR': (mild_outliers + extreme_outliers) / len(data) * 100,\n",
    "                'Outlier_Pct_Z2': z_outliers_2 / len(data) * 100\n",
    "            })\n",
    "    \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary)\n",
    "        print(\"\\nOUTLIER SUMMARY:\")\n",
    "        display(outlier_df.round(2))\n",
    "        \n",
    "        # Plot box plots for columns with many outliers\n",
    "        high_outlier_cols = outlier_df[outlier_df['Outlier_Pct_IQR'] > 5]['Column'].tolist()\n",
    "        \n",
    "        if high_outlier_cols:\n",
    "            print(f\"\\nColumns with >5% outliers: {high_outlier_cols}\")\n",
    "            \n",
    "            n_cols = min(len(high_outlier_cols), 6)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(high_outlier_cols[:n_cols]):\n",
    "                data = train_df[col].dropna()\n",
    "                axes[i].boxplot(data)\n",
    "                axes[i].set_title(f'{col}\\n{outlier_df[outlier_df[\"Column\"]==col][\"Outlier_Pct_IQR\"].iloc[0]:.1f}% outliers')\n",
    "                axes[i].set_ylabel(col)\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for i in range(n_cols, len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle('Box Plots for Columns with High Outlier Percentage', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Insights & Next Steps {#insights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and insights\n",
    "print(\"EDA SUMMARY AND INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = [\n",
    "    \"ðŸ“Š DATASET OVERVIEW:\",\n",
    "    f\"   â€¢ Dataset shape: {train_df.shape if 'train' in datasets else 'Data not loaded'}\",\n",
    "    f\"   â€¢ Numeric columns: {len(numeric_cols) if 'train' in datasets else 'N/A'}\",\n",
    "    f\"   â€¢ Missing data: {train_df.isnull().sum().sum() if 'train' in datasets else 'N/A'} values\",\n",
    "    \"\",\n",
    "    \"ðŸ§ª MOLECULAR DATA:\",\n",
    "    f\"   â€¢ SMILES columns detected: {len(potential_smiles_cols) if 'train' in datasets and 'potential_smiles_cols' in locals() else 'Unknown'}\",\n",
    "    f\"   â€¢ RDKit available: {RDKIT_AVAILABLE}\",\n",
    "    f\"   â€¢ Mordred available: {MORDRED_AVAILABLE}\",\n",
    "    \"\",\n",
    "    \"ðŸŽ¯ TARGET ANALYSIS:\",\n",
    "    f\"   â€¢ Target columns identified: {len(target_cols) if 'target_cols' in locals() else 'Please specify manually'}\",\n",
    "    f\"   â€¢ Distribution analysis: {'Completed' if 'train' in datasets else 'Pending data load'}\",\n",
    "    \"\",\n",
    "    \"ðŸ” DATA QUALITY:\",\n",
    "    f\"   â€¢ Duplicate rows: {train_df.duplicated().sum() if 'train' in datasets else 'N/A'}\",\n",
    "    f\"   â€¢ Outlier analysis: {'Completed' if 'outlier_summary' in locals() else 'Pending'}\",\n",
    "    f\"   â€¢ Correlation analysis: {'Completed' if 'train' in datasets and len(numeric_cols) > 1 else 'Limited'}\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RECOMMENDED NEXT STEPS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "next_steps = [\n",
    "    \"1. ðŸ“¥ DATA PREPARATION:\",\n",
    "    \"   â€¢ Download complete competition dataset if not available\",\n",
    "    \"   â€¢ Identify and validate target variables\",\n",
    "    \"   â€¢ Handle missing values with appropriate strategies\",\n",
    "    \"   â€¢ Remove or transform extreme outliers\",\n",
    "    \"\",\n",
    "    \"2. ðŸ§ª MOLECULAR FEATURE ENGINEERING:\",\n",
    "    \"   â€¢ Generate RDKit molecular descriptors\",\n",
    "    \"   â€¢ Calculate Mordred extended descriptors\",\n",
    "    \"   â€¢ Create polymer-specific features\",\n",
    "    \"   â€¢ Compute molecular similarity matrices\",\n",
    "    \"\",\n",
    "    \"3. ðŸ”¬ ADVANCED ANALYSIS:\",\n",
    "    \"   â€¢ Perform feature selection using correlation and importance\",\n",
    "    \"   â€¢ Apply dimensionality reduction (PCA, t-SNE)\",\n",
    "    \"   â€¢ Cluster similar molecules\",\n",
    "    \"   â€¢ Analyze structure-property relationships\",\n",
    "    \"\",\n",
    "    \"4. ðŸ¤– MODEL DEVELOPMENT:\",\n",
    "    \"   â€¢ Establish baseline models (Ridge, Random Forest)\",\n",
    "    \"   â€¢ Implement advanced ML models (XGBoost, CatBoost)\",\n",
    "    \"   â€¢ Explore deep learning approaches (Graph Neural Networks)\",\n",
    "    \"   â€¢ Design cross-validation strategy\",\n",
    "    \"\",\n",
    "    \"5. ðŸ“‹ VALIDATION & SUBMISSION:\",\n",
    "    \"   â€¢ Implement robust validation framework\",\n",
    "    \"   â€¢ Create ensemble methods\",\n",
    "    \"   â€¢ Generate competition submissions\",\n",
    "    \"   â€¢ Monitor leaderboard performance\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"NOTEBOOK USAGE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"â€¢ Update file paths and column names based on actual competition data\")\n",
    "print(\"â€¢ Uncomment and modify sections as needed for your specific dataset\")\n",
    "print(\"â€¢ Add domain-specific analysis based on competition requirements\")\n",
    "print(\"â€¢ Use this as a template for systematic EDA approach\")\n",
    "print(\"\\nâœ… EDA Framework Ready - Customize for Your Data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}