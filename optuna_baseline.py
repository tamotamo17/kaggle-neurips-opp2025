#!/usr/bin/env python3
"""
Optuna-Optimized Baseline with Robust NaN Handling
================================================

This script uses Optuna for hyperparameter optimization while properly handling
NaN values that can cause optimization to fail.
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime
import optuna
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import joblib

warnings.filterwarnings('ignore')


def simple_features(smiles_list):
    """Create robust features that never produce NaN."""
    features = []
    for smiles in smiles_list:
        feat = {
            'length': len(smiles),
            'carbon_count': smiles.count('C') + smiles.count('c'),
            'oxygen_count': smiles.count('O') + smiles.count('o'),
            'nitrogen_count': smiles.count('N') + smiles.count('n'),
            'star_count': smiles.count('*'),
            'ring_count': smiles.count('c'),
            'branch_count': smiles.count('('),
            'double_bond_count': smiles.count('='),
            'complexity': len(set(smiles)),
            'polymer_ratio': smiles.count('*') / max(len(smiles), 1)
        }
        features.append(feat)
    
    df = pd.DataFrame(features)
    
    # Ensure no infinite or NaN values
    df = df.replace([np.inf, -np.inf], 0)
    df = df.fillna(0)
    
    return df


def robust_cv_score(X, y, params, cv_folds=3, random_state=42):
    """
    Calculate cross-validation score with robust NaN handling.
    Returns a valid score even if some folds fail.
    """
    
    # Clean data upfront
    valid_mask = y.notna()
    if valid_mask.sum() < 10:
        return 1.0  # Bad score for insufficient data
    
    X_clean = X[valid_mask]
    y_clean = y[valid_mask]
    
    # Ensure no NaN/inf in features
    X_clean = X_clean.replace([np.inf, -np.inf], 0).fillna(0)
    y_clean = y_clean.replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(y_clean) < 10:
        return 1.0
    
    # Final alignment
    min_len = min(len(X_clean), len(y_clean))
    X_clean = X_clean.iloc[:min_len]
    y_clean = y_clean.iloc[:min_len]
    
    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scores = []
    
    for train_idx, val_idx in kfold.split(X_clean):
        try:
            X_train = X_clean.iloc[train_idx]
            X_val = X_clean.iloc[val_idx]
            y_train = y_clean.iloc[train_idx]
            y_val = y_clean.iloc[val_idx]
            
            # Additional per-fold cleaning
            X_train = X_train.replace([np.inf, -np.inf], 0).fillna(0)
            X_val = X_val.replace([np.inf, -np.inf], 0).fillna(0)
            
            # Skip if any issues remain
            if (not np.isfinite(X_train.values).all() or 
                not np.isfinite(X_val.values).all() or
                not np.isfinite(y_train.values).all() or
                not np.isfinite(y_val.values).all()):
                continue
            
            # Train model
            model = lgb.LGBMRegressor(
                **params,
                verbose=-1,
                random_state=random_state
            )
            
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            # Predict and validate
            pred = model.predict(X_val)
            
            # Check for NaN predictions
            if np.isnan(pred).any() or np.isinf(pred).any():
                continue
            
            # Calculate score
            rmse = np.sqrt(mean_squared_error(y_val, pred))
            
            # Validate score
            if np.isfinite(rmse) and rmse > 0:
                scores.append(rmse)
                
        except Exception as e:
            # Skip failed folds
            continue
    
    if not scores:
        return 1.0  # Return bad score if all folds failed
    
    return np.mean(scores)


def objective(trial, X, y, target_name):
    """
    Optuna objective function with robust error handling.
    """
    
    # Sample hyperparameters
    params = {
        'num_leaves': trial.suggest_int('num_leaves', 10, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),
        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),
    }
    
    try:
        # Get CV score
        score = robust_cv_score(X, y, params, cv_folds=3)
        
        # Ensure score is valid
        if not np.isfinite(score) or score <= 0:
            return 1.0
            
        return score
        
    except Exception as e:
        print(f"Trial failed: {str(e)[:100]}")
        return 1.0  # Return bad score for failed trials


def main():
    print("=" * 60)
    print("OPTUNA-OPTIMIZED BASELINE WITH ROBUST NaN HANDLING")
    print("=" * 60)
    
    # Load data
    train_df = pd.read_csv('data/train.csv')
    test_df = pd.read_csv('data/test.csv')
    
    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
    
    print(f"Train: {train_df.shape}, Test: {test_df.shape}")
    
    # Create features
    print("\nCreating robust features...")
    train_features = simple_features(train_df['SMILES'])
    test_features = simple_features(test_df['SMILES'])
    
    print(f"Features: {train_features.shape[1]} columns")
    
    # Scale features for stability
    scaler = StandardScaler()
    train_features_scaled = pd.DataFrame(
        scaler.fit_transform(train_features),
        columns=train_features.columns
    )
    test_features_scaled = pd.DataFrame(
        scaler.transform(test_features),
        columns=test_features.columns
    )
    
    # Optimize each target
    best_params = {}
    final_models = {}
    predictions = {}
    
    for target in targets:
        if target not in train_df.columns:
            print(f"\n‚ùå {target}: Missing column")
            predictions[target] = np.zeros(len(test_features))
            continue
            
        print(f"\nüîç Optimizing {target}...")
        
        y_target = train_df[target]
        valid_count = y_target.notna().sum()
        
        print(f"  Valid samples: {valid_count}/{len(y_target)}")
        
        if valid_count < 20:
            print(f"  ‚ùå Insufficient data, using mean")
            mean_val = y_target.mean() if valid_count > 0 else 0.0
            predictions[target] = np.full(len(test_features), mean_val)
            continue
        
        # Create study
        study = optuna.create_study(direction='minimize')
        
        # Optimize with timeout and error handling
        try:
            study.optimize(
                lambda trial: objective(trial, train_features_scaled, y_target, target),
                n_trials=50,  # Moderate number of trials
                timeout=300,  # 5 minute timeout per target
                show_progress_bar=True
            )
            
            best_params[target] = study.best_params
            best_score = study.best_value
            
            print(f"  ‚úÖ Best RMSE: {best_score:.6f}")
            print(f"  Best params: {study.best_params}")
            
        except Exception as e:
            print(f"  ‚ùå Optimization failed: {str(e)[:100]}")
            # Use default params
            best_params[target] = {
                'num_leaves': 50,
                'max_depth': 6,
                'min_child_samples': 20,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'learning_rate': 0.1,
                'n_estimators': 100,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'min_child_weight': 1.0,
            }
        
        # Train final model
        print(f"  üöÄ Training final model...")
        
        valid_mask = y_target.notna()
        X_final = train_features_scaled[valid_mask]
        y_final = y_target[valid_mask]
        
        final_model = lgb.LGBMRegressor(
            **best_params[target],
            verbose=-1,
            random_state=42
        )
        
        final_model.fit(X_final, y_final)
        
        # Make predictions
        pred = final_model.predict(test_features_scaled)
        
        # Validate predictions
        if np.isnan(pred).any() or np.isinf(pred).any():
            print(f"  ‚ö†Ô∏è  NaN predictions detected, using fallback")
            pred = np.full(len(test_features), y_final.mean())
        
        final_models[target] = final_model
        predictions[target] = pred
        
        print(f"  ‚úÖ {target} complete")
    
    # Create submission
    print("\nüìÑ Creating submission...")
    submission = pd.DataFrame({'id': test_df['id']})
    for target in targets:
        submission[target] = predictions[target]
    
    print("Submission preview:")
    print(submission)
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    submission_path = f'submissions/optuna_baseline_{timestamp}.csv'
    import os
    os.makedirs('submissions', exist_ok=True)
    submission.to_csv(submission_path, index=False)
    
    # Save models and params
    artifacts = {
        'models': final_models,
        'best_params': best_params,
        'scaler': scaler,
        'targets': targets,
        'timestamp': timestamp
    }
    
    os.makedirs('models', exist_ok=True)
    joblib.dump(artifacts, f'models/optuna_baseline_{timestamp}.pkl')
    
    print(f"\n" + "=" * 60)
    print("‚úÖ OPTUNA OPTIMIZATION COMPLETE!")
    print("=" * 60)
    print("Results:")
    for target in targets:
        if target in best_params:
            print(f"  ‚Ä¢ {target}: Optimized successfully")
        else:
            print(f"  ‚Ä¢ {target}: Used fallback strategy")
    
    print(f"\nFiles saved:")
    print(f"  ‚Ä¢ Submission: {submission_path}")
    print(f"  ‚Ä¢ Models: models/optuna_baseline_{timestamp}.pkl")
    print("\nüéØ No more NaN errors in Optuna!")


if __name__ == "__main__":
    main()